{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Patient Survival Prediction</h1>\n",
    "<hr>\n",
    "<h2>Project 4</h2>\n",
    "<p>This project predicts the survival of pacients in a UCI.</p>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "Open dataset\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patients = pd.read_csv('..\\datasets\\dataset_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hospital_death</th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>elective_surgery</th>\n",
       "      <th>height</th>\n",
       "      <th>pre_icu_los_days</th>\n",
       "      <th>weight</th>\n",
       "      <th>apache_2_diagnosis</th>\n",
       "      <th>apache_3j_diagnosis</th>\n",
       "      <th>apache_post_operative</th>\n",
       "      <th>...</th>\n",
       "      <th>hepatic_failure</th>\n",
       "      <th>immunosuppression</th>\n",
       "      <th>leukemia</th>\n",
       "      <th>lymphoma</th>\n",
       "      <th>solid_tumor_with_metastasis</th>\n",
       "      <th>ethnicity_encoded</th>\n",
       "      <th>gender_encoded</th>\n",
       "      <th>icu_type_encoded</th>\n",
       "      <th>apache_3j_bodysystem_encoded</th>\n",
       "      <th>apache_2_bodysystem_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>22.730000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>180.3</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>73.9</td>\n",
       "      <td>113.0</td>\n",
       "      <td>502.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>27.420000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>0.927778</td>\n",
       "      <td>70.2</td>\n",
       "      <td>108.0</td>\n",
       "      <td>203.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>31.950000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>172.7</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>95.3</td>\n",
       "      <td>122.0</td>\n",
       "      <td>703.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>22.640000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>165.1</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>61.7</td>\n",
       "      <td>203.0</td>\n",
       "      <td>1206.03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>27.560000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>190.5</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>100.0</td>\n",
       "      <td>301.0</td>\n",
       "      <td>403.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88259</th>\n",
       "      <td>0.0</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>23.060250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>177.8</td>\n",
       "      <td>0.298611</td>\n",
       "      <td>72.9</td>\n",
       "      <td>113.0</td>\n",
       "      <td>501.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88260</th>\n",
       "      <td>0.0</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>47.179671</td>\n",
       "      <td>0.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>0.120139</td>\n",
       "      <td>158.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>501.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88261</th>\n",
       "      <td>0.0</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>27.236914</td>\n",
       "      <td>0.0</td>\n",
       "      <td>170.2</td>\n",
       "      <td>0.046528</td>\n",
       "      <td>78.9</td>\n",
       "      <td>123.0</td>\n",
       "      <td>702.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88262</th>\n",
       "      <td>0.0</td>\n",
       "      <td>62.341131</td>\n",
       "      <td>23.297481</td>\n",
       "      <td>0.0</td>\n",
       "      <td>154.9</td>\n",
       "      <td>0.081944</td>\n",
       "      <td>55.9</td>\n",
       "      <td>108.0</td>\n",
       "      <td>203.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88263</th>\n",
       "      <td>0.0</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>22.031250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>0.018056</td>\n",
       "      <td>56.4</td>\n",
       "      <td>304.0</td>\n",
       "      <td>1409.02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88264 rows × 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       hospital_death        age        bmi  elective_surgery  height  \\\n",
       "0                 0.0  68.000000  22.730000               0.0   180.3   \n",
       "1                 0.0  77.000000  27.420000               0.0   160.0   \n",
       "2                 0.0  25.000000  31.950000               0.0   172.7   \n",
       "3                 0.0  81.000000  22.640000               1.0   165.1   \n",
       "4                 0.0  67.000000  27.560000               0.0   190.5   \n",
       "...               ...        ...        ...               ...     ...   \n",
       "88259             0.0  75.000000  23.060250               0.0   177.8   \n",
       "88260             0.0  56.000000  47.179671               0.0   183.0   \n",
       "88261             0.0  48.000000  27.236914               0.0   170.2   \n",
       "88262             0.0  62.341131  23.297481               0.0   154.9   \n",
       "88263             0.0  82.000000  22.031250               1.0   160.0   \n",
       "\n",
       "       pre_icu_los_days  weight  apache_2_diagnosis  apache_3j_diagnosis  \\\n",
       "0              0.541667    73.9               113.0               502.01   \n",
       "1              0.927778    70.2               108.0               203.01   \n",
       "2              0.000694    95.3               122.0               703.03   \n",
       "3              0.000694    61.7               203.0              1206.03   \n",
       "4              0.000694   100.0               301.0               403.01   \n",
       "...                 ...     ...                 ...                  ...   \n",
       "88259          0.298611    72.9               113.0               501.06   \n",
       "88260          0.120139   158.0               113.0               501.05   \n",
       "88261          0.046528    78.9               123.0               702.01   \n",
       "88262          0.081944    55.9               108.0               203.01   \n",
       "88263          0.018056    56.4               304.0              1409.02   \n",
       "\n",
       "       apache_post_operative  ...  hepatic_failure  immunosuppression  \\\n",
       "0                        0.0  ...              0.0                0.0   \n",
       "1                        0.0  ...              0.0                0.0   \n",
       "2                        0.0  ...              0.0                0.0   \n",
       "3                        1.0  ...              0.0                0.0   \n",
       "4                        0.0  ...              0.0                0.0   \n",
       "...                      ...  ...              ...                ...   \n",
       "88259                    0.0  ...              0.0                0.0   \n",
       "88260                    0.0  ...              0.0                0.0   \n",
       "88261                    0.0  ...              0.0                0.0   \n",
       "88262                    0.0  ...              0.0                0.0   \n",
       "88263                    1.0  ...              0.0                0.0   \n",
       "\n",
       "       leukemia  lymphoma  solid_tumor_with_metastasis  ethnicity_encoded  \\\n",
       "0           0.0       0.0                          0.0                2.0   \n",
       "1           0.0       0.0                          0.0                2.0   \n",
       "2           0.0       0.0                          0.0                2.0   \n",
       "3           0.0       0.0                          0.0                2.0   \n",
       "4           0.0       0.0                          0.0                2.0   \n",
       "...         ...       ...                          ...                ...   \n",
       "88259       0.0       0.0                          1.0                2.0   \n",
       "88260       0.0       0.0                          0.0                2.0   \n",
       "88261       0.0       0.0                          0.0                2.0   \n",
       "88262       0.0       0.0                          0.0                2.0   \n",
       "88263       0.0       0.0                          0.0                2.0   \n",
       "\n",
       "       gender_encoded  icu_type_encoded  apache_3j_bodysystem_encoded  \\\n",
       "0                 1.0               2.0                           9.0   \n",
       "1                 0.0               5.0                           8.0   \n",
       "2                 0.0               5.0                           5.0   \n",
       "3                 0.0               2.0                           0.0   \n",
       "4                 1.0               5.0                           7.0   \n",
       "...               ...               ...                           ...   \n",
       "88259             1.0               3.0                           9.0   \n",
       "88260             0.0               5.0                           9.0   \n",
       "88261             1.0               5.0                           5.0   \n",
       "88262             0.0               5.0                           8.0   \n",
       "88263             0.0               5.0                           1.0   \n",
       "\n",
       "       apache_2_bodysystem_encoded  \n",
       "0                              0.0  \n",
       "1                              6.0  \n",
       "2                              3.0  \n",
       "3                              0.0  \n",
       "4                              4.0  \n",
       "...                            ...  \n",
       "88259                          0.0  \n",
       "88260                          0.0  \n",
       "88261                          3.0  \n",
       "88262                          6.0  \n",
       "88263                          1.0  \n",
       "\n",
       "[88264 rows x 104 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_patients"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "Modelling\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the dataset, split into X and y datasets and then split them again into trains and test\n",
    "X = df_patients.drop(['hospital_death'], axis=1)\n",
    "y = df_patients['hospital_death']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30,random_state=11, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61784, 103)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<P>Modelling the Neural Network</p>\n",
    "<hr>\n",
    "<p> 103 inputs, because is the size of the data set</p>\n",
    "<p>due the result is binary, the output is set in 1</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network\n",
    "#\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(260, input_dim=103, activation='relu'))\n",
    "nn_model.add(Dense(520, activation='relu'))\n",
    "nn_model.add(Dense(260, activation='relu'))\n",
    "nn_model.add(Dense(130, activation='relu'))\n",
    "nn_model.add(Dense(65, activation='relu'))\n",
    "nn_model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "966/966 [==============================] - 5s 4ms/step - loss: 0.0000e+00 - accuracy: 0.9138\n",
      "Epoch 2/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: 0.0000e+00 - accuracy: 0.9145\n",
      "Epoch 3/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: 0.0000e+00 - accuracy: 0.9145\n",
      "Epoch 4/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: 0.0000e+00 - accuracy: 0.9145\n",
      "Epoch 5/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: 0.0000e+00 - accuracy: 0.9145\n",
      "Epoch 6/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: 0.0000e+00 - accuracy: 0.9145\n",
      "Epoch 7/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: 0.0000e+00 - accuracy: 0.9145\n",
      "Epoch 8/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: 0.0000e+00 - accuracy: 0.9145\n",
      "Epoch 9/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: 0.0000e+00 - accuracy: 0.9145\n",
      "Epoch 10/100\n",
      "966/966 [==============================] - 4s 5ms/step - loss: 0.0000e+00 - accuracy: 0.9145\n",
      "Epoch 11/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: 0.0000e+00 - accuracy: 0.9145\n",
      "Epoch 12/100\n",
      "966/966 [==============================] - 8s 8ms/step - loss: 0.0000e+00 - accuracy: 0.9145\n",
      "Epoch 13/100\n",
      "966/966 [==============================] - 9s 9ms/step - loss: 0.0000e+00 - accuracy: 0.9145\n",
      "Epoch 14/100\n",
      "966/966 [==============================] - 10s 10ms/step - loss: 0.0000e+00 - accuracy: 0.9145\n",
      "Epoch 15/100\n",
      "966/966 [==============================] - 10s 11ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 16/100\n",
      "966/966 [==============================] - 7s 7ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 17/100\n",
      "966/966 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 18/100\n",
      "966/966 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 19/100\n",
      "966/966 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 20/100\n",
      "966/966 [==============================] - 7s 8ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 21/100\n",
      "966/966 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 22/100\n",
      "966/966 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 23/100\n",
      "966/966 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 24/100\n",
      "966/966 [==============================] - 6s 6ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 25/100\n",
      "966/966 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 26/100\n",
      "966/966 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 27/100\n",
      "966/966 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 28/100\n",
      "966/966 [==============================] - 5s 6ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 29/100\n",
      "966/966 [==============================] - 6s 6ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 30/100\n",
      "966/966 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 31/100\n",
      "966/966 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 32/100\n",
      "966/966 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 33/100\n",
      "966/966 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 34/100\n",
      "966/966 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 35/100\n",
      "966/966 [==============================] - 5s 6ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 36/100\n",
      "966/966 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 37/100\n",
      "966/966 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 38/100\n",
      "966/966 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 39/100\n",
      "966/966 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 40/100\n",
      "966/966 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 41/100\n",
      "966/966 [==============================] - 6s 6ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 42/100\n",
      "966/966 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 43/100\n",
      "966/966 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 44/100\n",
      "966/966 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 45/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 46/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 47/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 48/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 49/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 50/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 51/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 52/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 53/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 54/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 55/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 56/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 57/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 58/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 59/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 60/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 61/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 62/100\n",
      "966/966 [==============================] - 3s 3ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 63/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 64/100\n",
      "966/966 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 65/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 66/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 67/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 68/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 69/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 70/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 71/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 72/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 73/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 74/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 75/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 76/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 77/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 78/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 79/100\n",
      "966/966 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 80/100\n",
      "966/966 [==============================] - 6s 6ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 81/100\n",
      "966/966 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 82/100\n",
      "966/966 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 83/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 84/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 85/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 86/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 87/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 88/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 89/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 90/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 91/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 92/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 93/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 94/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 95/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 96/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 97/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 98/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 99/100\n",
      "966/966 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.9145\n",
      "Epoch 100/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145\n"
     ]
    }
   ],
   "source": [
    "##Time to train!!\n",
    "history = nn_model.fit(X_train, y_train, epochs=100, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "828/828 [==============================] - 2s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "#predicting\n",
    "y_pred = nn_model.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Visualization of losses and accuracies</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], 'accuracy': [0.9138450026512146, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649, 0.9144924283027649]}\n"
     ]
    }
   ],
   "source": [
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val_accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hmorales\\Documents\\Personal\\TMLC\\Patient-survival-prediction\\notebooks\\ps_prediction_modelling.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hmorales/Documents/Personal/TMLC/Patient-survival-prediction/notebooks/ps_prediction_modelling.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hmorales/Documents/Personal/TMLC/Patient-survival-prediction/notebooks/ps_prediction_modelling.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hmorales/Documents/Personal/TMLC/Patient-survival-prediction/notebooks/ps_prediction_modelling.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(history\u001b[39m.\u001b[39;49mhistory[\u001b[39m'\u001b[39;49m\u001b[39mval_accuracy\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hmorales/Documents/Personal/TMLC/Patient-survival-prediction/notebooks/ps_prediction_modelling.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m'\u001b[39m\u001b[39mModel accuracy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hmorales/Documents/Personal/TMLC/Patient-survival-prediction/notebooks/ps_prediction_modelling.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m'\u001b[39m\u001b[39mAccuracy\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_accuracy'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
