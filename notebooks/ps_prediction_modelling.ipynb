{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Patient Survival Prediction</h1>\n",
    "<hr>\n",
    "<h2>Project 4</h2>\n",
    "<p>This project predicts the survival of pacients in a UCI.</p>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                            classification_report,\n",
    "                            roc_auc_score, roc_curve, auc, precision_recall_curve,\n",
    "                            confusion_matrix)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "Open dataset\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patients = pd.read_csv('../datasets/dataset_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hospital_death</th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>elective_surgery</th>\n",
       "      <th>height</th>\n",
       "      <th>pre_icu_los_days</th>\n",
       "      <th>weight</th>\n",
       "      <th>apache_2_diagnosis</th>\n",
       "      <th>apache_3j_diagnosis</th>\n",
       "      <th>apache_post_operative</th>\n",
       "      <th>...</th>\n",
       "      <th>hepatic_failure</th>\n",
       "      <th>immunosuppression</th>\n",
       "      <th>leukemia</th>\n",
       "      <th>lymphoma</th>\n",
       "      <th>solid_tumor_with_metastasis</th>\n",
       "      <th>ethnicity_encoded</th>\n",
       "      <th>gender_encoded</th>\n",
       "      <th>icu_type_encoded</th>\n",
       "      <th>apache_3j_bodysystem_encoded</th>\n",
       "      <th>apache_2_bodysystem_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>22.730000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>180.3</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>73.9</td>\n",
       "      <td>113.0</td>\n",
       "      <td>502.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>27.420000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>0.927778</td>\n",
       "      <td>70.2</td>\n",
       "      <td>108.0</td>\n",
       "      <td>203.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>31.950000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>172.7</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>95.3</td>\n",
       "      <td>122.0</td>\n",
       "      <td>703.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>22.640000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>165.1</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>61.7</td>\n",
       "      <td>203.0</td>\n",
       "      <td>1206.03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>27.560000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>190.5</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>100.0</td>\n",
       "      <td>301.0</td>\n",
       "      <td>403.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>57.450000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>165.1</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>156.6</td>\n",
       "      <td>108.0</td>\n",
       "      <td>203.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>25.710000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>175.3</td>\n",
       "      <td>0.060417</td>\n",
       "      <td>79.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>107.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>28.257052</td>\n",
       "      <td>1.0</td>\n",
       "      <td>154.9</td>\n",
       "      <td>0.004861</td>\n",
       "      <td>67.8</td>\n",
       "      <td>303.0</td>\n",
       "      <td>1304.08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>27.382812</td>\n",
       "      <td>1.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>1.271528</td>\n",
       "      <td>70.1</td>\n",
       "      <td>218.0</td>\n",
       "      <td>1505.02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>38.189067</td>\n",
       "      <td>1.0</td>\n",
       "      <td>172.7</td>\n",
       "      <td>1.376389</td>\n",
       "      <td>113.9</td>\n",
       "      <td>303.0</td>\n",
       "      <td>1304.05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   hospital_death   age        bmi  elective_surgery  height  \\\n",
       "0             0.0  68.0  22.730000               0.0   180.3   \n",
       "1             0.0  77.0  27.420000               0.0   160.0   \n",
       "2             0.0  25.0  31.950000               0.0   172.7   \n",
       "3             0.0  81.0  22.640000               1.0   165.1   \n",
       "4             0.0  67.0  27.560000               0.0   190.5   \n",
       "5             0.0  59.0  57.450000               0.0   165.1   \n",
       "6             0.0  50.0  25.710000               0.0   175.3   \n",
       "7             0.0  72.0  28.257052               1.0   154.9   \n",
       "8             0.0  80.0  27.382812               1.0   160.0   \n",
       "9             0.0  81.0  38.189067               1.0   172.7   \n",
       "\n",
       "   pre_icu_los_days  weight  apache_2_diagnosis  apache_3j_diagnosis  \\\n",
       "0          0.541667    73.9               113.0               502.01   \n",
       "1          0.927778    70.2               108.0               203.01   \n",
       "2          0.000694    95.3               122.0               703.03   \n",
       "3          0.000694    61.7               203.0              1206.03   \n",
       "4          0.000694   100.0               301.0               403.01   \n",
       "5          0.000694   156.6               108.0               203.01   \n",
       "6          0.060417    79.0               112.0               107.01   \n",
       "7          0.004861    67.8               303.0              1304.08   \n",
       "8          1.271528    70.1               218.0              1505.02   \n",
       "9          1.376389   113.9               303.0              1304.05   \n",
       "\n",
       "   apache_post_operative  ...  hepatic_failure  immunosuppression  leukemia  \\\n",
       "0                    0.0  ...              0.0                0.0       0.0   \n",
       "1                    0.0  ...              0.0                0.0       0.0   \n",
       "2                    0.0  ...              0.0                0.0       0.0   \n",
       "3                    1.0  ...              0.0                0.0       0.0   \n",
       "4                    0.0  ...              0.0                0.0       0.0   \n",
       "5                    0.0  ...              0.0                0.0       0.0   \n",
       "6                    0.0  ...              0.0                0.0       0.0   \n",
       "7                    1.0  ...              0.0                1.0       0.0   \n",
       "8                    1.0  ...              0.0                0.0       1.0   \n",
       "9                    1.0  ...              0.0                0.0       0.0   \n",
       "\n",
       "   lymphoma  solid_tumor_with_metastasis  ethnicity_encoded  gender_encoded  \\\n",
       "0       0.0                          0.0                2.0             1.0   \n",
       "1       0.0                          0.0                2.0             0.0   \n",
       "2       0.0                          0.0                2.0             0.0   \n",
       "3       0.0                          0.0                2.0             0.0   \n",
       "4       0.0                          0.0                2.0             1.0   \n",
       "5       0.0                          0.0                2.0             0.0   \n",
       "6       0.0                          0.0                6.0             1.0   \n",
       "7       0.0                          0.0                3.0             0.0   \n",
       "8       0.0                          0.0                2.0             0.0   \n",
       "9       0.0                          0.0                2.0             1.0   \n",
       "\n",
       "   icu_type_encoded  apache_3j_bodysystem_encoded  apache_2_bodysystem_encoded  \n",
       "0               2.0                           9.0                          0.0  \n",
       "1               5.0                           8.0                          6.0  \n",
       "2               5.0                           5.0                          3.0  \n",
       "3               2.0                           0.0                          0.0  \n",
       "4               5.0                           7.0                          4.0  \n",
       "5               5.0                           8.0                          6.0  \n",
       "6               0.0                           0.0                          0.0  \n",
       "7               5.0                           8.0                          6.0  \n",
       "8               5.0                           7.0                          4.0  \n",
       "9               4.0                           8.0                          6.0  \n",
       "\n",
       "[10 rows x 104 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_patients.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "Modelling\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the dataset, split into X and y datasets and then split them again into trains and test\n",
    "X = df_patients.drop(['hospital_death'], axis=1)\n",
    "y = df_patients['hospital_death']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30,random_state=11, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split Datasets\n",
      "==============\n",
      "1. X_train :(61784, 103)\n",
      "2. y_train :(61784,)\n",
      "1. X_test :(26480, 103)\n",
      "2. y_test :(26480,)\n"
     ]
    }
   ],
   "source": [
    "#let's see how split the datasets are\n",
    "print ('Split Datasets')\n",
    "print ('==============')\n",
    "print(f'1. X_train :{X_train.shape}')\n",
    "print(f'2. y_train :{y_train.shape}')\n",
    "print(f'1. X_test :{X_test.shape}')\n",
    "print(f'2. y_test :{y_test.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<P>Modelling the Neural Network</p>\n",
    "<hr>\n",
    "<p> 103 inputs, because is the size of the data set</p>\n",
    "<p>due the result is binary, the output is set in 1</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network\n",
    "#\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(260, input_dim=103, activation='relu'))\n",
    "nn_model.add(Dense(520, activation='relu'))\n",
    "nn_model.add(Dense(260, activation='relu'))\n",
    "nn_model.add(Dense(130, activation='relu'))\n",
    "nn_model.add(Dense(65, activation='relu'))\n",
    "nn_model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use test data as validation data and can check the accuracies after every epoch. This will give us an insight into overfitting at the time of training only and we can take steps before the completion of all epochs. That's why the using of <b>validation_data</b> tuple in the parameters of model.fit method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "966/966 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 2/100\n",
      "966/966 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 3/100\n",
      "966/966 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 4/100\n",
      "966/966 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 5/100\n",
      "966/966 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 6/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 7/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 8/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 9/100\n",
      "966/966 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 10/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 11/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 12/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 13/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 14/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 15/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 16/100\n",
      "966/966 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 17/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 18/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 19/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 20/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 21/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 22/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 23/100\n",
      "966/966 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 24/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 25/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 26/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 27/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 28/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 29/100\n",
      "966/966 [==============================] - 6s 6ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 30/100\n",
      "966/966 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 31/100\n",
      "966/966 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 32/100\n",
      "966/966 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 33/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 34/100\n",
      "966/966 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 35/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 36/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 37/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 38/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 39/100\n",
      "966/966 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 40/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 41/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 42/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 43/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 44/100\n",
      "966/966 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 45/100\n",
      "966/966 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 46/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 47/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 48/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 49/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 50/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 51/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 52/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 53/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 54/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 55/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 56/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 57/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 58/100\n",
      "966/966 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 59/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 60/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 61/100\n",
      "966/966 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 62/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 63/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 64/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 65/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 66/100\n",
      "966/966 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 67/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 68/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 69/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 70/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 71/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 72/100\n",
      "966/966 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 73/100\n",
      "966/966 [==============================] - 5s 6ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 74/100\n",
      "966/966 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 75/100\n",
      "966/966 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 76/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 77/100\n",
      "966/966 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 78/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 79/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 80/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 81/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 82/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 83/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 84/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 85/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 86/100\n",
      "966/966 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 87/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 88/100\n",
      "966/966 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 89/100\n",
      "966/966 [==============================] - 4s 5ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 90/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 91/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 92/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 93/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 94/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 95/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 96/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 97/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 98/100\n",
      "966/966 [==============================] - 4s 4ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 99/100\n",
      "966/966 [==============================] - 5s 5ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 100/100\n",
      "966/966 [==============================] - 6s 7ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n"
     ]
    }
   ],
   "source": [
    "##Time to train!!\n",
    "history = nn_model.fit(X_train, y_train,validation_data = (X_test,y_test), epochs=100, batch_size=64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "history variable keeps values of every iteration, I gonna use them to visualize them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what the heck!!!! something goes wrong!!! \n",
    "let's go to review the features or rescale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an instance for MinMaxScaler\n",
    "myScaler = MinMaxScaler()\n",
    "X_train_std = myScaler.fit_transform(X_train)\n",
    "X_test_std = myScaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to make both datasets equals, let's name the colums ... yeah equal.\n",
    "X_train_std = pd.DataFrame(X_train_std, columns= X_train.columns)\n",
    "X_test_std = pd.DataFrame(X_test_std, columns= X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split Datasets\n",
      "==============\n",
      "1. X_train :(61784, 103)\n",
      "1. X_test :(26480, 103)\n"
     ]
    }
   ],
   "source": [
    "#let's see how split the datasets are\n",
    "print ('Split Datasets')\n",
    "print ('==============')\n",
    "print(f'1. X_train :{X_train_std.shape}')\n",
    "print(f'1. X_test :{X_test_std.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hmorales/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: SyntaxWarning:\n",
      "\n",
      "In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962/966 [============================>.] - ETA: 0s - loss: 0.0000e+00 - accuracy: 0.9136"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hmorales/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176: SyntaxWarning:\n",
      "\n",
      "In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "966/966 [==============================] - 16s 13ms/step - loss: 0.0000e+00 - accuracy: 0.9136 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 2/100\n",
      "966/966 [==============================] - 13s 14ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 3/100\n",
      "966/966 [==============================] - 13s 13ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 4/100\n",
      "966/966 [==============================] - 12s 12ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 5/100\n",
      "966/966 [==============================] - 13s 13ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 6/100\n",
      "966/966 [==============================] - 13s 13ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 7/100\n",
      "966/966 [==============================] - 12s 12ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 8/100\n",
      "966/966 [==============================] - 13s 14ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 9/100\n",
      "966/966 [==============================] - 13s 13ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 10/100\n",
      "966/966 [==============================] - 13s 14ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 11/100\n",
      "966/966 [==============================] - 13s 13ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 12/100\n",
      "966/966 [==============================] - 13s 14ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 13/100\n",
      "966/966 [==============================] - 13s 14ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 14/100\n",
      "966/966 [==============================] - 13s 14ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 15/100\n",
      "966/966 [==============================] - 13s 14ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 16/100\n",
      "966/966 [==============================] - 13s 14ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 17/100\n",
      "966/966 [==============================] - 14s 14ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 18/100\n",
      "966/966 [==============================] - 14s 14ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 19/100\n",
      "966/966 [==============================] - 14s 14ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 20/100\n",
      "966/966 [==============================] - 13s 14ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 21/100\n",
      "966/966 [==============================] - 14s 15ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 22/100\n",
      "966/966 [==============================] - 14s 14ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 23/100\n",
      "966/966 [==============================] - 14s 14ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 24/100\n",
      "966/966 [==============================] - 13s 14ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 25/100\n",
      "966/966 [==============================] - 13s 14ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 26/100\n",
      "966/966 [==============================] - 14s 14ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 27/100\n",
      "966/966 [==============================] - 13s 14ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 28/100\n",
      "966/966 [==============================] - 14s 14ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 29/100\n",
      "966/966 [==============================] - 13s 14ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 30/100\n",
      "966/966 [==============================] - 13s 14ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 31/100\n",
      "966/966 [==============================] - 13s 14ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 32/100\n",
      "966/966 [==============================] - 14s 15ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 33/100\n",
      "966/966 [==============================] - 13s 14ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 34/100\n",
      "966/966 [==============================] - 13s 14ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 35/100\n",
      "966/966 [==============================] - 13s 14ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 36/100\n",
      "966/966 [==============================] - 13s 14ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 37/100\n",
      "966/966 [==============================] - 14s 14ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 38/100\n",
      "966/966 [==============================] - 13s 14ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 39/100\n",
      "966/966 [==============================] - 13s 14ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 40/100\n",
      "966/966 [==============================] - 14s 15ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 41/100\n",
      "966/966 [==============================] - 14s 14ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 42/100\n",
      "966/966 [==============================] - 14s 14ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 43/100\n",
      "966/966 [==============================] - 14s 14ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 44/100\n",
      "966/966 [==============================] - 14s 14ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 45/100\n",
      "966/966 [==============================] - 14s 14ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 46/100\n",
      "966/966 [==============================] - 14s 15ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 47/100\n",
      "966/966 [==============================] - 14s 14ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 48/100\n",
      "966/966 [==============================] - 14s 14ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 49/100\n",
      "966/966 [==============================] - 14s 14ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 50/100\n",
      "966/966 [==============================] - 14s 14ms/step - loss: 0.0000e+00 - accuracy: 0.9145 - val_loss: 0.0000e+00 - val_accuracy: 0.9145\n",
      "Epoch 51/100\n",
      "966/966 [==============================] - 14s 14ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 52/100\n",
      "966/966 [==============================] - 14s 14ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 53/100\n",
      "966/966 [==============================] - 14s 15ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 54/100\n",
      "966/966 [==============================] - 13s 14ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 55/100\n",
      "966/966 [==============================] - 14s 15ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 56/100\n",
      "966/966 [==============================] - 15s 15ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 57/100\n",
      "966/966 [==============================] - 14s 14ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 58/100\n",
      "966/966 [==============================] - 14s 14ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 59/100\n",
      "966/966 [==============================] - 14s 14ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 60/100\n",
      "966/966 [==============================] - 14s 14ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 61/100\n",
      "966/966 [==============================] - 14s 14ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 62/100\n",
      "966/966 [==============================] - 14s 15ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 63/100\n",
      "966/966 [==============================] - 14s 14ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 64/100\n",
      "966/966 [==============================] - 14s 15ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 65/100\n",
      "966/966 [==============================] - 13s 14ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 66/100\n",
      "966/966 [==============================] - 14s 15ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 67/100\n",
      "966/966 [==============================] - 14s 14ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 68/100\n",
      "966/966 [==============================] - 14s 15ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 69/100\n",
      "966/966 [==============================] - 14s 14ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 70/100\n",
      "966/966 [==============================] - 15s 16ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 71/100\n",
      "966/966 [==============================] - 14s 14ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 72/100\n",
      "966/966 [==============================] - 14s 15ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 73/100\n",
      "966/966 [==============================] - 14s 15ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 74/100\n",
      "966/966 [==============================] - 14s 14ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 75/100\n",
      "966/966 [==============================] - 14s 15ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 76/100\n",
      "966/966 [==============================] - 14s 14ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 77/100\n",
      "966/966 [==============================] - 14s 15ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 78/100\n",
      "966/966 [==============================] - 14s 15ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 79/100\n",
      "966/966 [==============================] - 14s 14ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 80/100\n",
      "966/966 [==============================] - 12s 12ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 81/100\n",
      "966/966 [==============================] - 11s 12ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 82/100\n",
      "966/966 [==============================] - 12s 12ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 83/100\n",
      "966/966 [==============================] - 14s 14ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 84/100\n",
      "966/966 [==============================] - 13s 14ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 85/100\n",
      "966/966 [==============================] - 12s 12ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 86/100\n",
      "966/966 [==============================] - 11s 12ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 87/100\n",
      "966/966 [==============================] - 13s 13ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 88/100\n",
      "966/966 [==============================] - 11s 12ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 89/100\n",
      "966/966 [==============================] - 12s 13ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 90/100\n",
      "966/966 [==============================] - 11s 12ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 91/100\n",
      "966/966 [==============================] - 11s 12ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 92/100\n",
      "966/966 [==============================] - 12s 12ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 93/100\n",
      "966/966 [==============================] - 13s 14ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 94/100\n",
      "966/966 [==============================] - 13s 14ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 95/100\n",
      "966/966 [==============================] - 14s 14ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 96/100\n",
      "966/966 [==============================] - 14s 14ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 97/100\n",
      "966/966 [==============================] - 14s 14ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 98/100\n",
      "966/966 [==============================] - 14s 14ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 99/100\n",
      "966/966 [==============================] - 14s 14ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n",
      "Epoch 100/100\n",
      "966/966 [==============================] - 14s 14ms/step - loss: nan - accuracy: 0.9145 - val_loss: nan - val_accuracy: 0.9145\n"
     ]
    }
   ],
   "source": [
    "#ok, now I can run again the model, same parameters, but a new dataset\n",
    "history = nn_model.fit(X_train_std, y_train,validation_data = (X_test_std,y_test), epochs=100, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#I have no a good feeling about this... the values are quite similar. anyway, let's go to visualize\n",
    "#plot\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " That's mean the scale is not causing the mistake. let's try to change the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "828/828 [==============================] - 4s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "#predicting\n",
    "y_pred = nn_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "828/828 [==============================] - 4s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_std = nn_model.predict(X_test_std)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to use ROC score to evaluate de model. So firstly i need to get False Positive  Rates and True positive rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/hmorales/Cursos/TMLC/Patient-survival-prediction/notebooks/ps_prediction_modelling.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hmorales/Cursos/TMLC/Patient-survival-prediction/notebooks/ps_prediction_modelling.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#I'm going to take just the first 50 values of y_pred\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hmorales/Cursos/TMLC/Patient-survival-prediction/notebooks/ps_prediction_modelling.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m false_positive_rate1, true_positive_rate1, threshold1 \u001b[39m=\u001b[39m roc_curve(y_test[:\u001b[39m50\u001b[39;49m], y_pred_std[:\u001b[39m50\u001b[39;49m])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:992\u001b[0m, in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mroc_curve\u001b[39m(\n\u001b[1;32m    905\u001b[0m     y_true, y_score, \u001b[39m*\u001b[39m, pos_label\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, drop_intermediate\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    906\u001b[0m ):\n\u001b[1;32m    907\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute Receiver operating characteristic (ROC).\u001b[39;00m\n\u001b[1;32m    908\u001b[0m \n\u001b[1;32m    909\u001b[0m \u001b[39m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[39m    array([1.8 , 0.8 , 0.4 , 0.35, 0.1 ])\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 992\u001b[0m     fps, tps, thresholds \u001b[39m=\u001b[39m _binary_clf_curve(\n\u001b[1;32m    993\u001b[0m         y_true, y_score, pos_label\u001b[39m=\u001b[39;49mpos_label, sample_weight\u001b[39m=\u001b[39;49msample_weight\n\u001b[1;32m    994\u001b[0m     )\n\u001b[1;32m    996\u001b[0m     \u001b[39m# Attempt to drop thresholds corresponding to points in between and\u001b[39;00m\n\u001b[1;32m    997\u001b[0m     \u001b[39m# collinear with other points. These are always suboptimal and do not\u001b[39;00m\n\u001b[1;32m    998\u001b[0m     \u001b[39m# appear on a plotted ROC curve (and thus do not affect the AUC).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1003\u001b[0m     \u001b[39m# but does not drop more complicated cases like fps = [1, 3, 7],\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m     \u001b[39m# tps = [1, 2, 4]; there is no harm in keeping too many thresholds.\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m     \u001b[39mif\u001b[39;00m drop_intermediate \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(fps) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:755\u001b[0m, in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    753\u001b[0m y_score \u001b[39m=\u001b[39m column_or_1d(y_score)\n\u001b[1;32m    754\u001b[0m assert_all_finite(y_true)\n\u001b[0;32m--> 755\u001b[0m assert_all_finite(y_score)\n\u001b[1;32m    757\u001b[0m \u001b[39m# Filter out zero-weighted samples, as they should not impact the result\u001b[39;00m\n\u001b[1;32m    758\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:190\u001b[0m, in \u001b[0;36massert_all_finite\u001b[0;34m(X, allow_nan, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39massert_all_finite\u001b[39m(\n\u001b[1;32m    165\u001b[0m     X,\n\u001b[1;32m    166\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m     input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    170\u001b[0m ):\n\u001b[1;32m    171\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Throw a ValueError if X contains NaN or infinity.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \n\u001b[1;32m    173\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39m        documentation.\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m     _assert_all_finite(\n\u001b[1;32m    191\u001b[0m         X\u001b[39m.\u001b[39;49mdata \u001b[39mif\u001b[39;49;00m sp\u001b[39m.\u001b[39;49missparse(X) \u001b[39melse\u001b[39;49;00m X,\n\u001b[1;32m    192\u001b[0m         allow_nan\u001b[39m=\u001b[39;49mallow_nan,\n\u001b[1;32m    193\u001b[0m         estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[1;32m    194\u001b[0m         input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[1;32m    195\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m estimator_name \u001b[39mand\u001b[39;00m input_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    145\u001b[0m     \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    148\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m     )\n\u001b[0;32m--> 161\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "false_positive_rate1, true_positive_rate1, threshold1 = roc_curve(y_test, y_pred_std)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Visualization of losses and accuracies</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n"
     ]
    }
   ],
   "source": [
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val_accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hmorales\\Documents\\Personal\\TMLC\\Patient-survival-prediction\\notebooks\\ps_prediction_modelling.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hmorales/Documents/Personal/TMLC/Patient-survival-prediction/notebooks/ps_prediction_modelling.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hmorales/Documents/Personal/TMLC/Patient-survival-prediction/notebooks/ps_prediction_modelling.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hmorales/Documents/Personal/TMLC/Patient-survival-prediction/notebooks/ps_prediction_modelling.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(history\u001b[39m.\u001b[39;49mhistory[\u001b[39m'\u001b[39;49m\u001b[39mval_accuracy\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hmorales/Documents/Personal/TMLC/Patient-survival-prediction/notebooks/ps_prediction_modelling.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m'\u001b[39m\u001b[39mModel accuracy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hmorales/Documents/Personal/TMLC/Patient-survival-prediction/notebooks/ps_prediction_modelling.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m'\u001b[39m\u001b[39mAccuracy\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_accuracy'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
